# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
import pytest
import numpy as np

from slangpy import DeviceType, float3
from slangpy.types import Tensor, diffPair
from slangpy.types.tensor import Tensor
from slangpy.testing import helpers

# pyright: reportOptionalMemberAccess=false, reportArgumentType=false

POLYNOMIAL_OUT_PARAM = r"""
[Differentiable]
void polynomial(float a, float b, out float result) {
    result = a * a + b + 1;
}
"""

POLYNOMIAL_WITH_RETURN_VALUE_DOT_SLANG = r"""
[Differentiable]
float polynomial(float a, float b) {
    return a * a + b + 1;
}
"""

POLYNOMIAL_RETURN_VALUE_ND = r"""
float polynomial(float a, float b) {
    return a * a + b + 1;
}
"""

POLYNOMIAL_V3 = r"""
[Differentiable]
float3 polynomial(float3 a, float3 b) {
    return a * a + b + 1;
}
"""


def python_eval_polynomial(a: float, b: float) -> float:
    return a * a + b + 1


def python_eval_polynomial_a_deriv(a: float, b: float) -> float:
    return 2 * a


def python_eval_polynomial_b_deriv(a: float, b: float) -> float:
    return 1


@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_call_none_differentiable(device_type: DeviceType):

    device = helpers.get_device(device_type)
    function = helpers.create_function_from_module(device, "polynomial", POLYNOMIAL_RETURN_VALUE_ND)

    a = 5.0
    b = 10.0
    res = function(a, b)
    assert res == python_eval_polynomial(a, b)

    with pytest.raises(
        Exception,
        match="Could not call function 'polynomial': Function is not differentiable",
    ):
        function.bwds(a, b, res)


@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_call_with_none_diff_scalars(device_type: DeviceType):

    device = helpers.get_device(device_type)
    function = helpers.create_function_from_module(
        device, "polynomial", POLYNOMIAL_WITH_RETURN_VALUE_DOT_SLANG
    )

    a = 5.0
    b = 10.0
    res = function(a, b)
    assert res == python_eval_polynomial(a, b)

    function.bwds(a, b, res)


@pytest.mark.skip("Awaiting auto-diff changes")
@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_call_with_diff_scalars(device_type: DeviceType):

    device = helpers.get_device(device_type)
    kernel_eval_polynomial = helpers.create_function_from_module(
        device, "polynomial", POLYNOMIAL_WITH_RETURN_VALUE_DOT_SLANG
    )

    a = 5.0
    b = 10.0
    res = kernel_eval_polynomial(a, b)
    expected = python_eval_polynomial(a, b)
    assert res == expected

    res_diff = diffPair(d=1.0)

    a_diff = diffPair(p=a)
    kernel_eval_polynomial.bwds(a_diff, b, res_diff)
    exprected_grad = python_eval_polynomial_a_deriv(a, b)
    assert a_diff.grad == exprected_grad

    b_diff = diffPair(p=b)
    kernel_eval_polynomial.bwds(a, b_diff, res_diff)
    exprected_grad = python_eval_polynomial_b_deriv(a, b)
    assert b_diff.grad == exprected_grad

    a_diff = diffPair(p=a)
    b_diff = diffPair(p=b)
    kernel_eval_polynomial.bwds(a_diff, b_diff, res_diff)
    exprected_grad = python_eval_polynomial_a_deriv(a, b)
    assert a_diff.grad == exprected_grad
    exprected_grad = python_eval_polynomial_b_deriv(a, b)
    assert b_diff.grad == exprected_grad


@pytest.mark.skip("Awaiting auto-diff changes")
@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_call_with_diff_pairs(device_type: DeviceType):

    device = helpers.get_device(device_type)
    kernel_eval_polynomial = helpers.create_function_from_module(
        device, "polynomial", POLYNOMIAL_WITH_RETURN_VALUE_DOT_SLANG
    )

    a = diffPair(5.0)
    b = diffPair(10.0)
    res = kernel_eval_polynomial(a, b)
    expected = python_eval_polynomial(a.primal, b.primal)
    assert res == expected

    kernel_eval_polynomial.bwds(a, b, diffPair(d=1.0))
    exprected_grad = python_eval_polynomial_a_deriv(a.primal, b.primal)
    assert a.grad == exprected_grad
    exprected_grad = python_eval_polynomial_b_deriv(a.primal, b.primal)
    assert b.grad == exprected_grad


@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_call_with_buffers(device_type: DeviceType):

    device = helpers.get_device(device_type)
    kernel_eval_polynomial = helpers.create_function_from_module(
        device, "polynomial", POLYNOMIAL_WITH_RETURN_VALUE_DOT_SLANG
    ).return_type(Tensor)

    a = Tensor.empty(shape=(32,), device=device, dtype=float).with_grads()
    a.storage.copy_from_numpy(np.random.rand(32).astype(np.float32))

    b = Tensor.empty(shape=(32,), device=device, dtype=float).with_grads()
    b.storage.copy_from_numpy(np.random.rand(32).astype(np.float32))

    res: Tensor = kernel_eval_polynomial(a, b)

    a_data = a.storage.to_numpy().view(np.float32)
    b_data = b.storage.to_numpy().view(np.float32)
    expected = python_eval_polynomial(a_data, b_data)
    res_data = res.to_numpy().view(np.float32)

    assert np.allclose(res_data, expected)

    res_w_grads = res.with_grads()
    res_w_grads.grad.storage.copy_from_numpy(
        np.ones(res_w_grads.shape.as_tuple(), dtype=np.float32)
    )

    kernel_eval_polynomial.bwds(a, b, res_w_grads)
    a_grad_data = a.grad.to_numpy().view(np.float32)
    b_grad_data = b.grad.to_numpy().view(np.float32)

    exprected_grad = python_eval_polynomial_a_deriv(a_data, b_data)
    assert np.allclose(a_grad_data, exprected_grad)

    exprected_grad = python_eval_polynomial_b_deriv(a_data, b_data)
    assert np.allclose(b_grad_data, exprected_grad)


@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_vec3_call_with_buffers(device_type: DeviceType):

    device = helpers.get_device(device_type)
    kernel_eval_polynomial = helpers.create_function_from_module(
        device, "polynomial", POLYNOMIAL_V3
    ).return_type(Tensor)

    a = Tensor.empty(
        shape=(32,),
        device=device,
        dtype=float3,
    ).with_grads()
    helpers.write_ndbuffer_from_numpy(a, np.random.rand(32 * 3).astype(np.float32), 3)

    b = Tensor.empty(
        shape=(32,),
        device=device,
        dtype=float3,
    ).with_grads()
    helpers.write_ndbuffer_from_numpy(b, np.random.rand(32 * 3).astype(np.float32), 3)

    res: Tensor = kernel_eval_polynomial(a, b)
    a_data = helpers.read_ndbuffer_from_numpy(a).reshape(-1, 3)
    b_data = helpers.read_ndbuffer_from_numpy(b).reshape(-1, 3)

    expected = python_eval_polynomial(a_data, b_data)
    res_data = helpers.read_ndbuffer_from_numpy(res).reshape(-1, 3)

    assert np.allclose(res_data, expected)

    res = res.with_grads()
    helpers.write_ndbuffer_from_numpy(res.grad, np.ones(32 * 3).astype(np.float32), 3)

    kernel_eval_polynomial.bwds(a, b, res)

    # TODO: https://github.com/shader-slang/slangpy/issues/118
    # We use ByteAddressBuffer to store the out grads, however, in the shader code, we use
    # `sizeof(T)` to calculate the offset of each element, which is wrong because sizeof(T)
    # is not guaranteed to be aligned on metal target. So we will just read the raw data back.
    # The WAR solution is to provide a element_stride to shader. Slang will add intrinsic to
    # calculate the aligned stride in shader code.
    a_grad_data = a.grad.storage.to_numpy().view(np.float32)[0 : 32 * 3].reshape(-1, 3)
    b_grad_data = b.grad.storage.to_numpy().view(np.float32)[0 : 32 * 3].reshape(-1, 3)

    exprected_grad = python_eval_polynomial_a_deriv(a_data, b_data)
    assert np.allclose(a_grad_data, exprected_grad)

    exprected_grad = python_eval_polynomial_b_deriv(a_data, b_data)
    assert np.allclose(b_grad_data, exprected_grad)


@pytest.mark.parametrize("device_type", helpers.DEFAULT_DEVICE_TYPES)
def test_vec3_call_with_buffers_soa(device_type: DeviceType):

    device = helpers.get_device(device_type)
    kernel_eval_polynomial = helpers.create_function_from_module(
        device, "polynomial", POLYNOMIAL_V3
    ).return_type(Tensor)

    a_x = Tensor.empty(
        shape=(32,),
        device=device,
        dtype=float,
    ).with_grads()
    a_x.storage.copy_from_numpy(np.random.rand(32).astype(np.float32))

    a_y = Tensor.empty(
        shape=(32,),
        device=device,
        dtype=float,
    ).with_grads()
    a_y.storage.copy_from_numpy(np.random.rand(32).astype(np.float32))

    a_z = Tensor.empty(
        shape=(32,),
        device=device,
        dtype=float,
    ).with_grads()
    a_z.storage.copy_from_numpy(np.random.rand(32).astype(np.float32))

    b = Tensor.empty(
        shape=(32,),
        device=device,
        dtype=float3,
    ).with_grads()
    b.storage.copy_from_numpy(np.random.rand(32 * 3).astype(np.float32))

    res: Tensor = kernel_eval_polynomial({"x": a_x, "y": a_y, "z": a_z}, b)

    a_x_data = a_x.storage.to_numpy().view(np.float32).reshape(-1, 1)
    a_y_data = a_y.storage.to_numpy().view(np.float32).reshape(-1, 1)
    a_z_data = a_z.storage.to_numpy().view(np.float32).reshape(-1, 1)
    a_data = np.column_stack((a_x_data, a_y_data, a_z_data))
    b_data = helpers.read_ndbuffer_from_numpy(b).reshape(-1, 3)
    expected = python_eval_polynomial(a_data, b_data)
    res_data = helpers.read_ndbuffer_from_numpy(res).reshape(-1, 3)

    assert np.allclose(res_data, expected)

    res = res.with_grads()
    helpers.write_ndbuffer_from_numpy(res.grad, np.ones(32 * 3).astype(np.float32), 3)

    kernel_eval_polynomial.bwds({"x": a_x, "y": a_y, "z": a_z}, b, res)
    a_x_grad_data = a_x.grad.storage.to_numpy().view(np.float32).reshape(-1, 1)
    a_y_grad_data = a_y.grad.storage.to_numpy().view(np.float32).reshape(-1, 1)
    a_z_grad_data = a_z.grad.storage.to_numpy().view(np.float32).reshape(-1, 1)

    a_grad_data = np.column_stack((a_x_grad_data, a_y_grad_data, a_z_grad_data))

    # TODO: https://github.com/shader-slang/slangpy/issues/118
    # We use ByteAddressBuffer to store the out grads, however, in the shader code, we use
    # `sizeof(T)` to calculate the offset of each element, which is wrong because sizeof(T)
    # is not guaranteed to be aligned on metal target. So we will just read the raw data back.
    # The WAR solution is to provide a element_stride to shader. Slang will add intrinsic to
    # calculate the aligned stride in shader code.
    b_grad_data = b.grad.storage.to_numpy().view(np.float32)[0 : 32 * 3]

    exprected_grad = python_eval_polynomial_a_deriv(a_data, b_data)
    assert np.allclose(a_grad_data, exprected_grad)

    exprected_grad = python_eval_polynomial_b_deriv(a_data, b_data)
    assert np.allclose(b_grad_data, exprected_grad)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
